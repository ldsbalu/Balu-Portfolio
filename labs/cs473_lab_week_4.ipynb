{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "KL divergence is one of the most commonly used concepts in machine learning. Here, we'll explore the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "---\n",
        "## Exercise #1: Symmetry    \n",
        "\n",
        "KL divergence is a *measure*, but not a *metric*. This means that while it satisfies some properties of things like a distance metric, it does not satisfy all of them.\n",
        "\n",
        "For example, KL divergence is NOT symmetric. First, implement a function that calculates the KL divergence between two discrete distributions. Then,cCraft an example to demonstrate that it is not symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j1m2KIHShNdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d536cf-93f7-422b-eff0-8d532e1022a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution 'a': [0.1 0.9]\n",
            "Distribution 'b': [0.8 0.2]\n",
            "KL(a || b) = 1.1457\n",
            "KL(b || a) = 1.3627\n",
            "Is KL(a || b) == KL(b || a)? No\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kl(a,b):\n",
        "    # a is a n-dimensional distribution\n",
        "    # b is a n-dimensional distribution\n",
        "    #\n",
        "    # return: KL(a||b)\n",
        "\n",
        "    # your code here\n",
        "    return np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
        "\n",
        "# find an example where kl(a,b) != kl(b,a)\n",
        "a = np.array([0.1, 0.9])\n",
        "b = np.array([0.8, 0.2])\n",
        "kl_a_b = kl(a, b)\n",
        "kl_b_a = kl(b, a)\n",
        "\n",
        "print(f\"Distribution 'a': {a}\")\n",
        "print(f\"Distribution 'b': {b}\")\n",
        "\n",
        "\n",
        "print(f\"KL(a || b) = {kl_a_b:.4f}\")\n",
        "print(f\"KL(b || a) = {kl_b_a:.4f}\")\n",
        "\n",
        "print(f\"Is KL(a || b) == KL(b || a)? {'Yes' if np.isclose(kl_a_b, kl_b_a) else 'No'}\")\n",
        "\n",
        "# etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zRdG8R0YzMo"
      },
      "source": [
        "---\n",
        "## Exercise #2: Triangle inequality\n",
        "\n",
        "Another property that KL divergence does not satisfy is the triangle inequality, which states that\n",
        "\n",
        "kl(a,c) >= kl(a,b)+kl(b,c)\n",
        "\n",
        "Prove that KL divergence does not satisfy the triangle inequality by crafting a counter-example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z6M6rRCYzMo",
        "outputId": "2793777a-8cd7-400a-bfdf-a73b23a4ca18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution 'a': [0.9 0.1]\n",
            "Distribution 'b': [0.5 0.5]\n",
            "Distribution 'c': [0.1 0.9]\n",
            "Direct Divergence D_KL(a || c)          = 1.7578\n",
            "Indirect Path D_KL(a || b) + D_KL(b || c) = 0.8789\n",
            "Does the triangle inequality hold? (a,c <= a,b + b,c)\n",
            "Result: False\n"
          ]
        }
      ],
      "source": [
        "a = np.array([0.9, 0.1])\n",
        "b = np.array([0.5, 0.5])\n",
        "c = np.array([0.1, 0.9])\n",
        "\n",
        "kl_a_c = kl(a, c)\n",
        "kl_a_b = kl(a, b)\n",
        "kl_b_c = kl(b, c)\n",
        "\n",
        "indirect_path_sum = kl_a_b + kl_b_c\n",
        "\n",
        "print(f\"Distribution 'a': {a}\")\n",
        "print(f\"Distribution 'b': {b}\")\n",
        "print(f\"Distribution 'c': {c}\")\n",
        "\n",
        "print(f\"Direct Divergence D_KL(a || c)          = {kl_a_c:.4f}\")\n",
        "print(f\"Indirect Path D_KL(a || b) + D_KL(b || c) = {indirect_path_sum:.4f}\")\n",
        "\n",
        "print(f\"Does the triangle inequality hold? (a,c <= a,b + b,c)\")\n",
        "print(f\"Result: {kl_a_c <= indirect_path_sum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG1NbIB0YzMp"
      },
      "source": [
        "---\n",
        "## Exercise #3: Proofs\n",
        "\n",
        "Prove that:\n",
        "\n",
        "1) kl(a,a) = 0\n",
        "2) kl(a,b) >= 0\n",
        "\n",
        "Extra credit:\n",
        "\n",
        "3) kl(a,b) = 0 iff a==b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUveqy6wYzMp"
      },
      "source": [
        "1.\n",
        "Defination of KL divergence - $$ D_{KL}(a || b) = \\sum_{i} a(i) \\log\\left(\\frac{a(i)}{b(i)}\\right) $$  \n",
        "To find $ D_{KL}(a || a)$, we should substitute b with a.\n",
        "$$ D_{KL}(a || a) = \\sum_{i} a(i) \\log\\left(\\frac{a(i)}{a(i)}\\right) $$\n",
        "if $ a(i) > 0 $ the fraction inside the logarithm becomes 1\n",
        "$$ D_{KL}(a || a) = \\sum_{i} a(i) \\log(1) $$ where $log(1) = 0$\n",
        "$$ D_{KL}(a || a) = \\sum_{i} a(i).0 = 0 $$\n",
        "Hence proved that $D_{KL}(a || a) = 0$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "Defination of KL divergence - $$ D_{KL}(a || b) = \\sum_{i} a(i) \\log\\left(\\frac{a(i)}{b(i)}\\right) $$  \n",
        "\n",
        "using log property $ log(\\frac{x}{y}) = -log(\\frac{y}{x})$ the equation becomes\n",
        "$$ D_{KL}(a || b) = -\\sum_{i} a(i) \\log\\left(\\frac{b(i)}{a(i)}\\right) $$   \n",
        "the above equation can be interpreted as expected value of $log(\\frac{b}{a})$\n",
        "$$ \\sum_{i} a(i) \\log\\left( \\frac{a(i)}{b(i)} \\right) = \\mathbb{E}_{a}\\left[\\log\\left(\\frac{a}{b}\\right)\\right] $$\n",
        "here apply Jensen's Inequality, Since $log(x)$ is a concave function.\n",
        "$$ \\mathbb{E}_a\\left[ \\log\\left( \\frac{a}{b} \\right) \\right] \\leq \\log \\left( \\mathbb{E}_a\\left[ \\frac{a}{b} \\right] \\right) $$\n",
        "after evaluating\n",
        "$$ \\mathbb{E}_a\\left[\\frac{b}{a}\\right] = \\sum_{i} a(i) \\cdot \\frac{b(i)}{a(i)} = \\sum_{i} {b(i)} $$\n",
        "Since b is a probability distribution, ùö∫ b(i) = 1.\n",
        "$$ \\mathbb{E}_a\\left[ \\log\\left(\\frac{b}{a}\\right) \\right] \\leq \\log(1) = 0 $$\n",
        "using log property $ log(\\frac{x}{y}) = -log(\\frac{y}{x})$ the equation becomes\n",
        "$$D_{KL}(a || b) = - E_a\\left[\\log\\left(\\frac{b}{a}\\right)\\right] \\ge -(0) = 0 $$\n",
        "Hence prove that $D_{KL}(a || b) ‚â• 0$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g7AxUIcWA6ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "it asks us if and only if so it must be proved both sides.<br>\n",
        "part 1: \"if\" direction $ a=b, then D_{KL}(a || b) = 0$.\n",
        "$$ \\text{ If }a(i) = b(i) \\text{ for all } i, \\text{ then } D_{KL}(a || b) = D_{KL}(a || a) = 0.$$\n",
        "part 2: \"only if\" direction  $ \\text{ If } D_{KL}(a || b) = 0, \\text{ then }  a=b $. <br>\n",
        "using the equality $\\mathbb{E}[\\log(X)] \\leq \\log(\\mathbb{E}[X])$ and we know that $D_{KL}(a || b) = 0$ because of Jensen's inequality.\n",
        "$$ \\mathbb{E}_a\\left[ \\log\\left( \\frac{a}{b} \\right) \\right] = \\log \\left( \\mathbb{E}_a\\left[ \\frac{a}{b} \\right] \\right) $$\n",
        "For this equality to be true random variable that we are taking equal to constant 'c' ‚àÄ $a(i)>0$\n",
        "$$ \\frac{a(i)}{b(i)} = c \\implies a(i) = c \\cdot b(i) $$\n",
        "for finding  the value of c, we sum over all i and we know that both a and b probability distributions.\n",
        "$$ \\sum_i b(i) = \\sum_i c \\cdot a(i) $$\n",
        "$$  1 = c \\sum_i a(i)$$\n",
        "$$ 1 = c \\cdot 1 \\implies c = 1 $$\n",
        "Since c = 2, we can conclude that:\n",
        "$$ \\frac{a(i)}{b(i)} = 1 \\implies b(i) = a(i) \\implies a = b $$\n",
        "Hence proved that $ a=b, \\text{then } D_{KL}(a || b) = 0$."
      ],
      "metadata": {
        "id": "KXO7sHriA8N9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}