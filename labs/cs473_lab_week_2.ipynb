{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "Welcome to your first lab for CS 473, Advanced Machine Learning.\n",
        "\n",
        "In machine learning, models often predict *unnormalized log probabilities*. These must often be converted into regular probabilities.\n",
        "\n",
        "In this lab, you will explore the log-sum-exp function, which is described in the text (Sec. 2.5.4).  You will code up several variants of the function, and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "# Part 1: Logsumexp\n",
        "---\n",
        "## Setup: The Iris Dataset\n",
        "We'll begin by downloading the Iris dataset. The iris dataset is a simple, but very famous, dataset introduced to the world by RA Fisher (the “father” of modern statistics”) in 1939. The dataset has five columns:\n",
        "* sepal length (cm)\n",
        "* sepal width (cm)\n",
        "* petal length (cm)\n",
        "* petal width (cm)\n",
        "* class\n",
        "\n",
        "In order to get logits to play with, we'll first train a multinomial logistic regression model (Sec. 2.5.3).  This model naturally outputs logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "j1m2KIHShNdC"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "ds = datasets.load_dataset( \"scikit-learn/iris\" )\n",
        "\n",
        "df = pd.DataFrame( ds['train'] )\n",
        "\n",
        "X = np.array( df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']] )\n",
        "Y = np.array( LabelEncoder().fit_transform( df['Species'] ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "l5hV6PS0CwT8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression().fit(X,Y)\n",
        "\n",
        "W = model.coef_\n",
        "b = model.intercept_\n",
        "\n",
        "b = np.reshape( b, (3,1))\n",
        "\n",
        "logits = np.dot( W, X.T ) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Esta7gBS1v"
      },
      "source": [
        "---\n",
        "## Exercise 1: convert logits to probabilities\n",
        "\n",
        "Since our model outputs logits, they must be converted. To do this, we'll use the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "E3sgOg5oAvCv"
      },
      "outputs": [],
      "source": [
        "def softmax( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "    # use equation 2.99 (see also Eq. 2.94)\n",
        "\n",
        "    # your code here\n",
        "    m = np.max(logits, axis=0, keepdims=True)\n",
        "    exp_logits = np.exp(logits - m)\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=0, keepdims=True)\n",
        "    return probs\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjWqdG-yKh1t",
        "outputId": "948eaac0-6a97-418d-df36-fa48799bdeee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.982  , 0.01823, 0.     ], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "# print out test cases\n",
        "probs = softmax( logits )\n",
        "probs[:,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[:,120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZD4tuQTgBuW",
        "outputId": "d80531d6-5b19-48c7-fd15-64099692a1c9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.543e-06, 2.397e-02, 9.761e-01], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoXi9mq3BS1w"
      },
      "source": [
        "### test cases\n",
        "probs = softmax( logits )\n",
        "probs[:,0]\n",
        "#### array([9.81803910e-01, 1.81960759e-02, 1.43430317e-08])\n",
        "probs[:,120]\n",
        "#### array([5.49519371e-06, 2.38812718e-02, 9.76113233e-01])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q64wxOyBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 2: convert logits to probabilities\n",
        "\n",
        "Now, code up the logsumexp function.  What test cases should you use for this function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "zFzz8bqZBS1w"
      },
      "outputs": [],
      "source": [
        "def logsumexp( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "    # use equation 2.100\n",
        "\n",
        "    # your code here\n",
        "    m = np.max(logits, axis=0, keepdims=True)\n",
        "    sum_exp = np.sum(np.exp(logits - m), axis=0, keepdims=True)\n",
        "    log_probs = logits - (m + np.log(sum_exp))\n",
        "    probs = np.exp(log_probs)\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "7sGtB6lNBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e81129-d3a4-459a-b421-54c31f4d8fae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9805 , 0.01817, 0.     ], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "# test cases\n",
        "probs = logsumexp( logits )\n",
        "probs[:,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EciYRi0OKh1u"
      },
      "source": [
        "What should be printed??\n",
        "\n",
        "it should be printed same as soft max function but shifted it decimals\n",
        "\n",
        "array([0.9805 , 0.01817, 0.     ], dtype=float16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9B45o8BS1w"
      },
      "source": [
        "---\n",
        "## Exercise 3: explore underflow / overlow\n",
        "\n",
        "First, code up a function that compares two distributions. This can be anything you want; you may consider things like the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "REO87dlmBS1w"
      },
      "outputs": [],
      "source": [
        "def compare_probs( probs1, probs2 ):\n",
        "    # your code here\n",
        "    return np.mean((probs1 - probs2) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "3A85oOGUBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe1e39e-bff6-43fe-f4b8-f0736ef645f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float16(3.6e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "probs1 = softmax( logits )\n",
        "probs2 = logsumexp( logits )\n",
        "compare_probs( probs1, probs2 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNMrfHJBS1w"
      },
      "source": [
        "Now, see what happens if you add (or subtract) a constant from logits. How big must the constant be before things start going haywire?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "rYmP1Jj7BS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "42c8722f-6db3-464d-a9d9-6e586788141a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=     0: MSE = 3.58e-07\n",
            "C=   100: MSE = 1.04e-04\n",
            "C=  1000: MSE = 2.62e-03\n",
            "C= 10000: MSE = 1.67e-01\n",
            "C=100000.0: MSE = nan\n",
            "C=  -100: MSE = 1.04e-04\n",
            "C= -1000: MSE = 2.62e-03\n",
            "C=-10000: MSE = 1.67e-01\n",
            "C=-100000.0: MSE = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1768883176.py:3: RuntimeWarning: overflow encountered in cast\n",
            "  probs1 = softmax(logits + C)\n",
            "/tmp/ipython-input-660115556.py:10: RuntimeWarning: invalid value encountered in subtract\n",
            "  exp_logits = np.exp(logits - m)\n",
            "/tmp/ipython-input-1768883176.py:4: RuntimeWarning: overflow encountered in cast\n",
            "  probs2 = logsumexp(logits + C)\n"
          ]
        }
      ],
      "source": [
        "constants = [0, 100, 1000, 10000, 1e5, -100, -1000, -10000, -1e5]\n",
        "for C in constants:\n",
        "        probs1 = softmax(logits + C)\n",
        "        probs2 = logsumexp(logits + C)\n",
        "        mse = compare_probs(probs1, probs2)\n",
        "        print(f\"C={C:6}: MSE = {mse:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaPkTeNBS1w"
      },
      "source": [
        "Now convert the logits to 16-bit precision, and re-run your experiments. Analyze the differences you see (2-3 sentences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "6oxJEheOBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4547f9cb-2e6d-44bc-e458-d15688f372b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=     0: MSE = 3.58e-07\n",
            "C=   100: MSE = 1.04e-04\n",
            "C=  1000: MSE = 2.62e-03\n",
            "C= 10000: MSE = 1.67e-01\n",
            "C=100000.0: MSE = nan\n",
            "C=  -100: MSE = 1.04e-04\n",
            "C= -1000: MSE = 2.62e-03\n",
            "C=-10000: MSE = 1.67e-01\n",
            "C=-100000.0: MSE = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2844424443.py:4: RuntimeWarning: overflow encountered in cast\n",
            "  probs1 = softmax(logits + C)\n",
            "/tmp/ipython-input-660115556.py:10: RuntimeWarning: invalid value encountered in subtract\n",
            "  exp_logits = np.exp(logits - m)\n",
            "/tmp/ipython-input-2844424443.py:5: RuntimeWarning: overflow encountered in cast\n",
            "  probs2 = logsumexp(logits + C)\n"
          ]
        }
      ],
      "source": [
        "logits = logits.astype( np.float16 )\n",
        "constants = [0, 100, 1000, 10000, 1e5, -100, -1000, -10000, -1e5]\n",
        "for C in constants:\n",
        "        probs1 = softmax(logits + C)\n",
        "        probs2 = logsumexp(logits + C)\n",
        "        mse = compare_probs(probs1, probs2)\n",
        "        print(f\"C={C:6}: MSE = {mse:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i70jvZNEKh1u"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "\n",
        "I observed if we add small constant until $\\pm$100 the output probabilites stay the same but larger constant they all become NAN Which causes Softmax to break due to overflow/underflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZs7xOKBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 4: cleanly compute log probabilities\n",
        "\n",
        "Sometimes, we want to compute log probabilities (which are different from logits), but we want to do so \"cleanly\", ie, while avoiding overflow / underflow. First, mathematically figure out what the log of the softmax is (ie, take the log of eq. 2.99), and then combine it with insights from coding up the logsumexp function. Hint: at the end of the day, you will simply shift each column by a per-column constant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "Laus2v79BS1x"
      },
      "outputs": [],
      "source": [
        "def log_logsumexp( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "\n",
        "    # your code here\n",
        "    m = np.max(logits, axis=0, keepdims=True)\n",
        "    logits_shifted = logits - m\n",
        "    log_sum_exp = m + np.log(np.sum(np.exp(logits_shifted), axis=0, keepdims=True))\n",
        "    return logits - log_sum_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "62UvuyhYBS1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3c04d1-0bb1-4d2a-bffe-674390823447"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -0.01953,  -4.008  , -18.06   ], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "probs = log_logsumexp( logits )\n",
        "probs[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "constants = [0, 100, 1000, 10000, 1e5, -100, -1000, -10000, -1e5]\n",
        "for C in constants:\n",
        "        probs1 = logsumexp(logits)\n",
        "        probs2 = log_logsumexp(logits + C)\n",
        "        mse = compare_probs(probs1, probs2)\n",
        "        print(f\"C={C:6}: MSE = {mse:.2e}\")\n",
        "\n",
        "logits = logits.astype( np.float16 )\n",
        "constants = [0, 100, 1000, 10000, 1e5, -100, -1000, -10000, -1e5]\n",
        "for C in constants:\n",
        "        probs1 = logsumexp(logits)\n",
        "        probs2 = log_logsumexp(logits + C)\n",
        "        mse = compare_probs(probs1, probs2)\n",
        "        print(f\"C={C:6}: MSE = {mse:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHzbRk0sdjYz",
        "outputId": "62777fbb-59c9-462c-ae45-390175750ed0"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=     0: MSE = 5.54e+01\n",
            "C=   100: MSE = 5.53e+01\n",
            "C=  1000: MSE = 5.50e+01\n",
            "C= 10000: MSE = 6.04e+01\n",
            "C=100000.0: MSE = nan\n",
            "C=  -100: MSE = 5.53e+01\n",
            "C= -1000: MSE = 5.50e+01\n",
            "C=-10000: MSE = 6.04e+01\n",
            "C=-100000.0: MSE = nan\n",
            "C=     0: MSE = 5.54e+01\n",
            "C=   100: MSE = 5.53e+01\n",
            "C=  1000: MSE = 5.50e+01\n",
            "C= 10000: MSE = 6.04e+01\n",
            "C=100000.0: MSE = nan\n",
            "C=  -100: MSE = 5.53e+01\n",
            "C= -1000: MSE = 5.50e+01\n",
            "C=-10000: MSE = 6.04e+01\n",
            "C=-100000.0: MSE = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2434273129.py:4: RuntimeWarning: overflow encountered in cast\n",
            "  probs2 = log_logsumexp(logits + C)\n",
            "/tmp/ipython-input-1720996561.py:9: RuntimeWarning: invalid value encountered in subtract\n",
            "  logits_shifted = logits - m\n",
            "/tmp/ipython-input-2434273129.py:12: RuntimeWarning: overflow encountered in cast\n",
            "  probs2 = log_logsumexp(logits + C)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxX04mG2zXOe"
      },
      "source": [
        "---\n",
        "# Part 2: Probability Fundamentals\n",
        "\n",
        "For the following exercises, you are encouraged to work both by hand and by code however makes the most sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xsg221sSR_4"
      },
      "source": [
        "## Exercise 1a: Joint Probability Distributions\n",
        "\n",
        "You are given the following two binary variables, X and Y, that can each take on the values 0 or 1. Assuming X and Y are independent, calculate the joint probability table (2x2 table for P(X, Y)). Display as a numpy array.\n",
        "\n",
        "P(X=0) = 0.6\n",
        "\n",
        "P(X=1) = 0.4\n",
        "\n",
        "P(Y=0) = 0.7\n",
        "\n",
        "P(Y=1) = 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "V7dgYZfrSQ22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8895dacc-e27d-4831-a6ce-e4167aae860b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.42 0.18]\n",
            " [0.28 0.12]]\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "P_X = np.array([0.6, 0.4])  # X=0, X=1\n",
        "P_Y = np.array([0.7, 0.3])  # Y=0, Y=1\n",
        "joint = np.outer(P_X, P_Y)\n",
        "print(joint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKat0t8w3qWY"
      },
      "source": [
        "Next, compute the following conditional probabilities:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "uoFygm9S3yPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57782b6d-820a-494b-9849-5c82d9fed41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(X=0|Y=0): 0.6\n",
            "P(X=0|Y=1): 0.6\n",
            "P(Y=0|X=0): 0.7\n",
            "P(Y=0|X=1): 0.7\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "p_x0_y0 = joint[0,0] / joint[:,0].sum()\n",
        "p_x0_y1 = joint[0,1] / joint[:,1].sum()\n",
        "p_y0_x0 = joint[0,0] / joint[0,:].sum()\n",
        "p_y0_x1 = joint[1,0] / joint[1,:].sum()\n",
        "print(\"P(X=0|Y=0):\", p_x0_y0)\n",
        "print(\"P(X=0|Y=1):\", p_x0_y1)\n",
        "print(\"P(Y=0|X=0):\", p_y0_x0)\n",
        "print(\"P(Y=0|X=1):\", p_y0_x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NT1EW_3Kh1u"
      },
      "source": [
        "Compare the result of these conditional probabilities to the original marginal probabilities given. What does this say about the relationship between variable dependence and using conditional probabilities? Write 1-2 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AcBmxcjKh1u"
      },
      "source": [
        "(Your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkEO2YEbKh1u"
      },
      "source": [
        "## Exercise 1b: Joint Probability Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz85nqMzKh1u"
      },
      "source": [
        "Now consider this joint distribution:\n",
        "\n",
        "|  | $Y = 0$ | $Y = 1$|\n",
        "| :------- | :------: | -------: |\n",
        "| $X = 0$  | 0.45  | 0.10  |\n",
        "| $X = 1$  | 0.25  | 0.20  |\n",
        "\n",
        "First, compute the marginals from the joint table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMjzVzPpKh1u",
        "outputId": "68a35323-fb89-465c-d4be-2f8ff03e2808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marginals: P(X): [0.55 0.45] P(Y): [0.7 0.3]\n"
          ]
        }
      ],
      "source": [
        "joint_prob_table = np.array([[0.45, 0.10],\n",
        "                             [0.25, 0.20]])\n",
        "\n",
        "# P(X=0) = ?\n",
        "# P(X=1) = ?\n",
        "# P(Y=0) = ?\n",
        "# P(Y=1) = ?\n",
        "\n",
        "# Your answer here\n",
        "joint = np.array([[0.45, 0.10], [0.25, 0.20]])\n",
        "PX = joint.sum(axis=1)\n",
        "PY = joint.sum(axis=0)\n",
        "\n",
        "print(\"Marginals: P(X):\", PX, \"P(Y):\", PY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHQbl-nLKh1u"
      },
      "source": [
        "Compute the same conditional probabilities as above:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYEuNietKh1u",
        "outputId": "bb7b0307-94f5-4c59-837d-36f5d8f9e851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(X=0|Y=0): 0.6428571428571429\n",
            "P(X=0|Y=1): 0.3333333333333333\n",
            "P(Y=0|X=0): 0.8181818181818181\n",
            "P(Y=0|X=1): 0.5555555555555556\n"
          ]
        }
      ],
      "source": [
        "# P(X=0|Y=0) = ?\n",
        "# P(X=0|Y=1) = ?\n",
        "# P(Y=0|X=0) = ?\n",
        "# P(Y=0|X=1) = ?\n",
        "\n",
        "# Your answer here\n",
        "p_x0_y0 = joint[0,0]/PY[0]\n",
        "p_x0_y1 = joint[0,1]/PY[1]\n",
        "p_y0_x0 = joint[0,0]/PX[0]\n",
        "p_y0_x1 = joint[1,0]/PX[1]\n",
        "print(\"P(X=0|Y=0):\", p_x0_y0)\n",
        "print(\"P(X=0|Y=1):\", p_x0_y1)\n",
        "print(\"P(Y=0|X=0):\", p_y0_x0)\n",
        "print(\"P(Y=0|X=1):\", p_y0_x1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOZEh_zYKh1v"
      },
      "source": [
        "Check if the independence property $P(X, Y) = P(X)P(Y)$ holds for any cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apkjA4WnKh1v",
        "outputId": "a7b55815-a057-4f62-f187-74c47be22b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check independence for (X=0,Y=1): 0.1 vs 0.16500000000000004\n"
          ]
        }
      ],
      "source": [
        "# Your answer here\n",
        "p_xy = joint[0,1]\n",
        "px_py = PX[0]*PY[1]\n",
        "print(\"Check independence for (X=0,Y=1):\", p_xy, \"vs\", px_py )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUGvFeqWKh1y"
      },
      "source": [
        "Compare P(X=0|Y=0) to P(X=0|Y=1), and discuss what this says about the dependence between these variables (1-2 sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYA8dbs6Kh1y"
      },
      "source": [
        "(Your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ypkpjoKh1y"
      },
      "source": [
        "P(X=0|Y=0) = 0.6429 ,\n",
        "P(X=0|Y=1) = 0.3333\n",
        "by this we can know that X and Y are both dependent variables since X=0 changes based on Y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOEoNBWTOUj"
      },
      "source": [
        "---\n",
        "## Exercise 2: Bayes Theorem\n",
        "\n",
        "After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive\n",
        "for a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you\n",
        "have the disease is 0.99, as is the probability of testing negative given that you don’t have the disease). The\n",
        "good news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you\n",
        "actually have the disease? (Show your calculations as well as giving the final result.)\n",
        "\n",
        "*Hint: write out the variables you know, and think about what you'll need to calculate to find the final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "fWWmJ6FbCyyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2c6933-3834-4857-cdac-638179bcac0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009803921568627442\n",
            "P(D|Positive) = 0.0098  (0.98%)\n"
          ]
        }
      ],
      "source": [
        "# Your answer/work here\n",
        "P_D = 1 / 10000           # probability of having disease\n",
        "P_notD = 1 - P_D          # probability of not having disease\n",
        "P_Pos_given_D = 0.99      # True positive\n",
        "P_Neg_given_notD = 0.99   # True negative\n",
        "\n",
        "P_Pos_given_notD = 1 - P_Neg_given_notD\n",
        "# law of total probability\n",
        "P_Pos = P_Pos_given_D * P_D + P_Pos_given_notD * P_notD\n",
        "#bayes theorem\n",
        "P_D_given_Pos = (P_Pos_given_D * P_D) / P_Pos\n",
        "print( P_D_given_Pos)\n",
        "print(f\"P(D|Positive) = {P_D_given_Pos:.4f}  ({P_D_given_Pos*100:.2f}%)\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}