{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "The concept of mutual information is powerful in theory, but in practice it can be difficult to estimate. In this lab, you will implement a conceptually simple way to estimate the mutual information between two random variables. It is known as the KSG estimator (from \"Kraskov–Stögbauer–Grassberger\").\n",
        "\n",
        "You will then use your estimator as part of a feature selection algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "---\n",
        "## Grading standards   \n",
        "\n",
        "Your notebook will be graded on the following:\n",
        "\n",
        "* 60% Correct implementation of KSG estimator\n",
        "* 20% Correct implementation of \"all_mutual_inf\"\n",
        "* 20% Correct selection of most informative columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFh2vnGf2hl"
      },
      "source": [
        "---\n",
        "## Description\n",
        "\n",
        "For this lab, we will use this dataset [lab_week_6_data.csv](https://github.com/wingated/cs473/blob/main/labs/data/lab_week_6_data.csv).\n",
        "\n",
        "Download the csv from the link above, then upload the csv to your current google colab runtime.\n",
        "\n",
        "Then begin by loading and preparing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ipQg2wmhf2hl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import digamma\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "df = pd.read_csv( \"lab_week_6_data.csv\" )\n",
        "\n",
        "X = df[['x1','x2','x3','x4','x5']]\n",
        "y = df['y']\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhXMiZnEf2hm"
      },
      "source": [
        "The KSG estimator is an elegant idea that's fairly easy to implement. The idea is to use an adaptive grid, where the grid depends on the local density of the data.  This is accomplished by calculating the k'th nearest neighbor for each point, and then defining a box around each point that just barely extends to include the k'th nearest neighbor. Importantly, the size of the box is also used in the marginal entropy calculations\n",
        "\n",
        "To get some intuition for the method, check out [this demonstration on Wolfram Alpha's site](https://demonstrations.wolfram.com/KraskovKSGEstimatorOfMutualInformation/).  Here is a screenshot of their UI:\n",
        "\n",
        "![Wolfram UI](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_6_image1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUS-fJnCf2hm"
      },
      "source": [
        "The algorithm is fairly straightforward.  We are given N samples from the joint distribution of p(X,Y). First, we calculate the *local mutual information* for each data point:\n",
        "\n",
        "1. For each datapoint $d_j = (x_j,y_j)$\n",
        "2. Find the k'th nearest neighbor (call this point $n_j$) -- you should use the infinity norm (max norm), and distance should be calculated using both coordinates (ie, don't just calculate distances using only the x or y coordinate)\n",
        "3. Count the number of points in x whose 'x-distance' fall within the distance to the k'th nearest neighbor (calculated above) of $d_j$ (ie, ignore y); call this number $n_{x{_j}}$.  Do the same for the points in y (these are the shaded gray boxes in the image); call it $n_{y{_j}}$.\n",
        "\n",
        "Then, we calculate the final mutual information by averaging over all of the datapoints and adding a few more digamma calls:\n",
        "\n",
        "![final mutual information](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_6_image3.png)\n",
        "\n",
        "where $\\Psi$ is the digamma function\n",
        "\n",
        "If you want more details on the implementation, see page 2 of [Estimating Mutual Information](https://arxiv.org/pdf/cond-mat/0305641)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cgWrDmVMf2hm"
      },
      "outputs": [],
      "source": [
        "def mutual_inf( X, Y, k ):\n",
        "    # X and Y are lists of length n of samples from p(X,Y)\n",
        "    #\n",
        "    # this function returns a single scalar\n",
        "    if X.ndim > 1 or Y.ndim > 1:\n",
        "        raise ValueError(\"X and Y must be 1-dimensional arrays.\")\n",
        "    if X.shape[0] != Y.shape[0]:\n",
        "        raise ValueError(\"X and Y must have the same number of samples.\")\n",
        "\n",
        "    n_samples = X.shape[0]\n",
        "    xy = np.c_[X,Y]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=k + 1, metric='chebyshev')\n",
        "    nn.fit(xy)\n",
        "    distances, _ = nn.kneighbors(xy)\n",
        "    radii = distances[:,-1]\n",
        "\n",
        "    n_x = np.zeros( n_samples )\n",
        "    n_y = np.zeros( n_samples )\n",
        "\n",
        "    for i in range( n_samples ):\n",
        "        r_i = radii[i]\n",
        "        # Count points whose x-distance is strictly less than the radius.\n",
        "        # This count includes the point itself.\n",
        "        n_x[i] = np.sum(np.abs(X - X[i]) < r_i)\n",
        "        # Count points whose y-distance is strictly less than the radius.\n",
        "        n_y[i] = np.sum(np.abs(Y - Y[i]) < r_i)\n",
        "\n",
        "        avg_digamma = np.mean(digamma(n_x) + digamma(n_y))\n",
        "        mi = digamma(k) - avg_digamma + digamma(n_samples)\n",
        "\n",
        "    return mi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd7rc8Vzf2hm"
      },
      "source": [
        "---\n",
        "## Part 2:\n",
        "\n",
        "Now that you can estimate mutual information between two variables, we're going to use your method in the context of a regression problem.\n",
        "\n",
        "Given our X,y data from the beginning of the lab, run your function and calculate the mutual information between every column of X and y.  This should result in a list of scores, that might look something like this:\n",
        "\n",
        "```python\n",
        "array([1.2641203602210282, 0.07974959311825458, 0.03321433628330617, 0.02213361087954091, 0.0, 0.0, 0.0])\n",
        "```\n",
        "\n",
        "If you wrap this in a single function (call it \"all_mutual_inf\"), then you can use scikit learn's feature selection algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WnHEuC-Gf2hn"
      },
      "outputs": [],
      "source": [
        "def all_mutual_inf( X, y ):\n",
        "    # return a list of mutual informations\n",
        "    # the list should be as long as the number of columns in X\n",
        "\n",
        "    # your code here\n",
        "    n_features = X.shape[1]\n",
        "    mi_scores = np.zeros(n_features)\n",
        "\n",
        "    for i in range(n_features):\n",
        "        # Calculate MI for the i-th feature column and y\n",
        "        mi_scores[i] = mutual_inf(X[:, i], y, k=3) # Using k=3 as a default\n",
        "\n",
        "    return mi_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OehN0Rwf2hn",
        "outputId": "ee812286-f212-4549-c1ab-386104582d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mutual Information Scores ---\n",
            "Feature 'x1': 1.5617\n",
            "Feature 'x2': 0.2381\n",
            "Feature 'x3': 0.1888\n",
            "Feature 'x4': 0.1728\n",
            "Feature 'x5': -0.0824\n",
            "\n",
            "--- Feature Selection Results ---\n",
            "Shape of original data: (100, 5)\n",
            "Shape of data after feature selection: (100, 2)\n",
            "\n",
            "The two most informative features are: x1, x2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "all_scores = all_mutual_inf(X, y)\n",
        "\n",
        "print(\"--- Mutual Information Scores ---\")\n",
        "for i, score in enumerate(all_scores):\n",
        "    print(f\"Feature 'x{i+1}': {score:.4f}\")\n",
        "\n",
        "selector = SelectKBest(all_mutual_inf, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "feature_names = np.array(['x1', 'x2', 'x3', 'x4', 'x5'])\n",
        "selected_features = feature_names[selected_indices]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMdch0i4f2hn"
      },
      "source": [
        "So: which X features are most informative about y?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of original data: {X.shape}\")\n",
        "print(f\"Shape of data after feature selection: {X_new.shape}\")\n",
        "print(f\"\\nThe two most informative features are: {', '.join(selected_features)}\")"
      ],
      "metadata": {
        "id": "BJBt8o0ljPI7",
        "outputId": "a840fad1-9cc1-4d66-9f4f-d93850426076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of original data: (100, 5)\n",
            "Shape of data after feature selection: (100, 2)\n",
            "\n",
            "The two most informative features are: x1, x2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6er8xof2hn"
      },
      "source": [
        "---\n",
        "## Hints\n",
        "\n",
        "The following functions may be useful to you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm2oRqVMf2hn"
      },
      "outputs": [],
      "source": [
        "scipy.special.digamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZFC1MZf2hn"
      },
      "source": [
        "For fun, you could also [read more about the KSG estimator](https://arxiv.org/pdf/1604.03006)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}