{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plUlj0v5i8aQ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/porterjenkins/byu-cs474/blob/master/lab5_generalization.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttC8f3uOlC2C"
      },
      "source": [
        "# Lab 5: Measuring Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQqEicJlNGa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "\n",
        "*   Understand how to accurately measure generalization performance of deep networks\n",
        "*   Gain intuition into the bias-variance trade-off and the double descent phenomenon\n",
        "*   Investigate properties of high dimensional spaces and better understand the ''curse of dimensionality''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDJ1RdGomDHj"
      },
      "source": [
        "## Deliverable\n",
        "\n",
        "You will turn in a completed version of notebook to Canvas/Learning Suite.  In various places you will see the words \"TO DO\". Follow the instructions at these places and write code to complete the instructions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrGvYA4pkxH-"
      },
      "source": [
        "## Notes\n",
        "You will not need a GPU instance for this lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joIT4SFmmeFp"
      },
      "source": [
        "## Q1) Understanding Generalization\n",
        "\n",
        "In this question, we will use the MNIST1D dataset (https://github.com/greydanus/mnist1d) to reproduce Figure 8.2 from Prince."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cSu7e4Jmtfm"
      },
      "outputs": [],
      "source": [
        "# Run this if you're in a Colab to install MNIST 1D repository\n",
        "%pip install git+https://github.com/greydanus/mnist1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z76SO4ymxbW"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mnist1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJ27aiZnRa-"
      },
      "source": [
        "Let's generate a training and test dataset using the MNIST1D code. The dataset gets saved as a .pkl file so it doesn't have to be regenerated each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh2VSEZHnSlo"
      },
      "outputs": [],
      "source": [
        "!mkdir ./sample_data\n",
        "\n",
        "args = mnist1d.data.get_dataset_args()\n",
        "data = mnist1d.data.get_dataset(args, path='./sample_data/mnist1d_data.pkl', download=False, regenerate=False)\n",
        "\n",
        "# The training and test input and outputs are in\n",
        "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFiZ1dY9nUcO"
      },
      "outputs": [],
      "source": [
        "D_i = 40    # Input dimensions\n",
        "D_k = 100   # Hidden dimensions\n",
        "D_o = 10    # Output dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpU3rh4KuPHv"
      },
      "source": [
        "### Part 1.a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBmIfg5veT9S"
      },
      "source": [
        "**TODO:** Define a PyTorch model with two hidden layers of size 100 And ReLU activations between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPSyIAj0rDqA"
      },
      "outputs": [],
      "source": [
        "# Your code here (see Figure 7.8 of book for help)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj_PwXFXekFA"
      },
      "source": [
        "**TODO:** Initialize the parameters with He initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S94b51qnoQU2"
      },
      "outputs": [],
      "source": [
        "def weights_init(layer_in):\n",
        "    # TODO: Replace this line (see figure 7.8 of Prince for help)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rf4hB4RoZpg"
      },
      "outputs": [],
      "source": [
        "# Call the function you just defined\n",
        "model.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbNIYtj7nZQj"
      },
      "outputs": [],
      "source": [
        "x_train = torch.tensor(data['x'].astype('float32'))\n",
        "y_train = torch.tensor(data['y'].transpose().astype('long'))\n",
        "x_test= torch.tensor(data['x_test'].astype('float32'))\n",
        "y_test = torch.tensor(data['y_test'].astype('long'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noejMuuZi8aU"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, n_epoch=50):\n",
        "\n",
        "    # Initialize model weights\n",
        "    model.apply(weights_init)\n",
        "\n",
        "    # store the loss and the % correct at each epoch\n",
        "    losses_train = np.zeros((n_epoch))\n",
        "    errors_train = np.zeros((n_epoch))\n",
        "    losses_test = np.zeros((n_epoch))\n",
        "    errors_test = np.zeros((n_epoch))\n",
        "\n",
        "    # choose cross entropy loss function (equation 5.24)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    # construct SGD optimizer and initialize learning rate and momentum\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
        "    # object that decreases learning rate by half every 10 epochs\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      # loop over batches\n",
        "      for i, batch in enumerate(data_loader):\n",
        "        # retrieve inputs and labels for this batch\n",
        "        x_batch, y_batch = batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass -- calculate model output\n",
        "        pred = model(x_batch)\n",
        "        # compute the loss\n",
        "        loss = loss_function(pred, y_batch)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # SGD update\n",
        "        optimizer.step()\n",
        "\n",
        "      # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "      pred_train = model(x_train)\n",
        "      pred_test = model(x_test)\n",
        "      _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "      _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "      errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "      errors_test[epoch]= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
        "      losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
        "      losses_test[epoch]= loss_function(pred_test, y_test).item()\n",
        "      print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  test loss {losses_test[epoch]:.6f}, test error {errors_test[epoch]:3.2f}')\n",
        "\n",
        "      # tell scheduler to consider updating learning rate\n",
        "      scheduler.step()\n",
        "\n",
        "    return errors_train, errors_test, losses_train, losses_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_TzmBOVnlSL"
      },
      "outputs": [],
      "source": [
        "def plot_results(errors_train, errors_test, losses_train, losses_test, n_epoch=50):\n",
        "\n",
        "    # Plot the results\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(errors_train,'r-',label='train')\n",
        "    ax.plot(errors_test,'b-',label='test')\n",
        "    ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "    ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "    ax.set_title('TrainError %3.2f, Test Error %3.2f'%(errors_train[-1],errors_test[-1]))\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the results\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(losses_train,'r-',label='train')\n",
        "    ax.plot(losses_test,'b-',label='test')\n",
        "    ax.set_xlim(0,n_epoch)\n",
        "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "    ax.set_title('Train loss %3.2f, Test loss %3.2f'%(losses_train[-1],losses_test[-1]))\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_c9RkaHi8aV"
      },
      "source": [
        "**TODO:** Train your model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ1JFA_Oi8aV"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DeFYlL2i8aV"
      },
      "source": [
        "**TODO**: Plot the resulting quantities:\n",
        "- Train errors\n",
        "- Test errors\n",
        "- Train losses\n",
        "- Test losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAxzCxIZi8aV"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slNQbGY8uYiX"
      },
      "source": [
        "### Part 1.b)\n",
        "\n",
        "Now let's increase the capacity of our model to have five hidden layers and re-run our experiment. Pay attention to what happens to the train and test curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSPCfIoTvU4g"
      },
      "source": [
        "**TODO:**\n",
        "Define a model with five hidden layers of size 100\n",
        "and ReLU activations between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy2gTmeruiz7"
      },
      "outputs": [],
      "source": [
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx0MKjx3vbLs"
      },
      "source": [
        "**TODO**: Train the model using the same code as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLPELEolvnxS"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3feLw5v7pd"
      },
      "source": [
        "**TODO**: Plot the results using the same code as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QV9y79Mv-Fk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKPGfSmuwcbS"
      },
      "source": [
        "### Part 1.c)\n",
        "\n",
        "Let's run one last experiment, this time with a simple linear model with **NO** hidden layers. In this case we will decrease the capacity of the model. Again, pay attention to what happens to the train and test curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDOtAsSYwtD2"
      },
      "source": [
        "**TODO:** Define a model with no hidden layers. This model should just be a linear layer that maps from the input dimension, `D_i` to the output dimension, `D_o`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpKrzL2Dw9Gt"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2KDpACWxJ43"
      },
      "source": [
        "**TODO**: Train the model using the same code as above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88LzTORexQfX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnmXf9ZVxUum"
      },
      "source": [
        "**TODO**: Plot the results using the same code as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w2DbeqWyNDr"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhYWl39Hn2kx"
      },
      "source": [
        "### Part 1.d: Discussion\n",
        "What did you observe as you changed the model capacity in your experiments? What are some measures you might include to improve generalization of your models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBw7TEVki8ad"
      },
      "outputs": [],
      "source": [
        "# Your response here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_qzCupZxv56"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWdgJdgGy163"
      },
      "source": [
        "## Q2:  Bias-variance Trade-off\n",
        "\n",
        "In this problem, we will investigate the bias-variance trade-off and reproduce the curves seen in Figure 8.8 in Prince."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biNd-0BmzB8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyyvXug7zSJT"
      },
      "source": [
        "Let's specify the true function that we are trying to estimate, defined on [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp_hhiFHzN1j"
      },
      "outputs": [],
      "source": [
        "def true_function(x):\n",
        "    y = np.exp(np.sin(x*(2*3.1413)))\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTz8KoI9zV20"
      },
      "source": [
        "Now let's generate some data point and add bit of noise to the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RibUkvvBzUkC"
      },
      "outputs": [],
      "source": [
        "def generate_data(n_data, sigma_y=0.3):\n",
        "    # Generate x values quasi uniformly\n",
        "    x = np.ones(n_data)\n",
        "    for i in range(n_data):\n",
        "        x[i] = np.random.uniform(i/n_data, (i+1)/n_data, 1)\n",
        "\n",
        "    # y value from running through function and adding noise\n",
        "    y = np.ones(n_data)\n",
        "    for i in range(n_data):\n",
        "        y[i] = true_function(x[i])\n",
        "        y[i] += np.random.normal(0, sigma_y, 1)\n",
        "    return x,y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc-ywywAzf6O"
      },
      "source": [
        "Let's draw the fitted function, together with uncertainty used to generate points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIPVYxWEzfJ5"
      },
      "outputs": [],
      "source": [
        "def plot_function(x_func, y_func, x_data=None,y_data=None, x_model = None, y_model =None, sigma_func = None, sigma_model=None):\n",
        "\n",
        "    fig,ax = plt.subplots()\n",
        "    ax.plot(x_func, y_func, 'k-')\n",
        "    if sigma_func is not None:\n",
        "      ax.fill_between(x_func, y_func-2*sigma_func, y_func+2*sigma_func, color='lightgray')\n",
        "\n",
        "    if x_data is not None:\n",
        "        ax.plot(x_data, y_data, 'o', color='#d18362')\n",
        "\n",
        "    if x_model is not None:\n",
        "        ax.plot(x_model, y_model, '-', color='#7fe7de')\n",
        "\n",
        "    if sigma_model is not None:\n",
        "      ax.fill_between(x_model, y_model-2*sigma_model, y_model+2*sigma_model, color='lightgray')\n",
        "\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_xlabel('Input, x')\n",
        "    ax.set_ylabel('Output, y')\n",
        "    plt.show()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p14Ig5cNzxru"
      },
      "source": [
        "Sample from the true function (no noise):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMBVcLwNzjmn"
      },
      "outputs": [],
      "source": [
        "x_func = np.linspace(0, 1.0, 100)\n",
        "y_func = true_function(x_func);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UyzfDRwz5e1"
      },
      "source": [
        "Generate some training data (with noise):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqnTSqluz-am"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "sigma_func = 0.3\n",
        "n_data = 15\n",
        "x_data,y_data = generate_data(n_data, sigma_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3wH2zDi0H5s"
      },
      "source": [
        "Plot the data with the functions defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GBGX5K60Lbz"
      },
      "outputs": [],
      "source": [
        "plot_function(x_func, y_func, x_data, y_data, sigma_func=sigma_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8SVgFH50MMs"
      },
      "outputs": [],
      "source": [
        "# Define model -- beta is a scalar and omega has size n_hidden,1\n",
        "def network(x, beta, omega):\n",
        "    # Retrieve number of hidden units\n",
        "    n_hidden = omega.shape[0]\n",
        "\n",
        "    y = np.zeros_like(x)\n",
        "    for c_hidden in range(n_hidden):\n",
        "        # Evaluate activations based on shifted lines (figure 8.4b-d)\n",
        "        line_vals =  x  - c_hidden/n_hidden\n",
        "        h =  line_vals * (line_vals > 0)\n",
        "        # Weight activations by omega parameters and sum\n",
        "        y = y + omega[c_hidden] * h\n",
        "    # Add bias, beta\n",
        "    y = y + beta\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A3Hwp462OcD"
      },
      "outputs": [],
      "source": [
        "# This fits the n_hidden+1 parameters (see fig 8.4a) in closed form.\n",
        "# If you have studied linear algebra, then you will know it is a least\n",
        "# squares solution of the form (A^TA)^-1A^Tb.  If you don't recognize that,\n",
        "# then just take it on trust that this gives you the best possible solution.\n",
        "def fit_model_closed_form(x,y,n_hidden):\n",
        "  n_data = len(x)\n",
        "  A = np.ones((n_data, n_hidden+1))\n",
        "  for i in range(n_data):\n",
        "      for j in range(1,n_hidden+1):\n",
        "          A[i,j] = x[i]-(j-1)/n_hidden\n",
        "          if A[i,j] < 0:\n",
        "              A[i,j] = 0;\n",
        "\n",
        "  beta_omega = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "\n",
        "  beta = beta_omega[0]\n",
        "  omega = beta_omega[1:]\n",
        "\n",
        "  return beta, omega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0pMEIav2SkA"
      },
      "outputs": [],
      "source": [
        "# Closed form solution\n",
        "beta, omega = fit_model_closed_form(x_data,y_data,n_hidden=3)\n",
        "\n",
        "# Get prediction for model across graph range\n",
        "x_model = np.linspace(0,1,100);\n",
        "y_model = network(x_model, beta, omega)\n",
        "\n",
        "# Draw the function and the model\n",
        "plot_function(x_func, y_func, x_data,y_data, x_model, y_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo9GjQKLftvu"
      },
      "source": [
        "### Part 2.a) Estimating the mean and variance of the model outputs, over many training runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6m0EzZa28Nl"
      },
      "source": [
        "**TODO:** Fill in the missing pieces of this function to tun the model many times with different datasets and return the mean and variance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSCIXEi2YqR"
      },
      "outputs": [],
      "source": [
        "def get_model_mean_variance(n_data, n_datasets, n_hidden, sigma_func):\n",
        "\n",
        "  # Create array that stores model results in rows\n",
        "  y_model_all = np.zeros((n_datasets, n_data))\n",
        "\n",
        "  for c_dataset in range(n_datasets):\n",
        "    # TODO -- Generate n_data x,y, pairs with standard deviation sigma_func\n",
        "    # Replace this line\n",
        "    x_data, y_data = None, None\n",
        "\n",
        "    # TODO -- Fit the model\n",
        "    # Replace this line:\n",
        "    beta, omega = None, None\n",
        "\n",
        "    # TODO -- Run the fitted model on x_data\n",
        "    # Replace this line\n",
        "    y_model = None\n",
        "\n",
        "    # Store the model results\n",
        "    y_model_all[c_dataset,:] = y_model\n",
        "\n",
        "  # Get mean and standard deviation of model\n",
        "  mean_model = np.mean(y_model_all,axis=0)\n",
        "  std_model = np.std(y_model_all,axis=0)\n",
        "\n",
        "  # Return the mean and standard deviation of the fitted model\n",
        "  return mean_model, std_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzMkq7KrfVpf"
      },
      "source": [
        "Let's generate N=100 random data sets, fit the model N=100 times and look the mean and variance. Here we will have 15 data points and 3 hidden units in our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVA-Fsek2awb"
      },
      "outputs": [],
      "source": [
        "n_datasets = 100\n",
        "n_data = 15\n",
        "sigma_func = 0.3\n",
        "n_hidden = 3\n",
        "\n",
        "# Get mean and variance of fitted model\n",
        "np.random.seed(1)\n",
        "mean_model, std_model = get_model_mean_variance(n_data, n_datasets, n_hidden, sigma_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKXyiLhufnKy"
      },
      "source": [
        "Plot the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_sVRJFM59Il"
      },
      "outputs": [],
      "source": [
        "x_model_grid = np.linspace(0, 1, len(mean_model))\n",
        "plot_function(x_func, y_func, x_model=x_model_grid, y_model=mean_model, sigma_model=std_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh4WZfS1eLCf"
      },
      "source": [
        "If you did this correctly, you can see that there that we observe both **bias** and **variance** in the model outputs. Here bias refers to the fact that we have some error from the model outputs and the true function (distance between cyan and black lines); variance refers to the gray region indicating there is a fair amount of variability in what the model outputs over each dataset it sees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwK2BJaTgfEf"
      },
      "source": [
        "### Part 2.b) Changing the amount of available data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmgb8vk9gkmq"
      },
      "source": [
        "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase the the number of training points to 100, `n_data=100`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TocjrVyMgjv-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfwYl0RFg5RB"
      },
      "source": [
        "**TODO**: Plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C22lkN1Fg6vN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ85NPNJhErP"
      },
      "source": [
        "**TODO**: What happened to the variance? Record your observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILpVwLAchNIE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZbtyZAdhU3n"
      },
      "source": [
        "### Part 2.c) Increasing the model capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PELiw9OmhgZI"
      },
      "source": [
        "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase set the number of hidden units to 12 and the number of training points to 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzObr8GtiHAC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HinKPGgQiQjd"
      },
      "source": [
        "**TODO**: Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qr9NuZFiUab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddt1HRY8iXSa"
      },
      "source": [
        "**TODO**: What happened to the bias? Record your observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STzzHzUXibS9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-q3uKaPibZd"
      },
      "source": [
        "### Part 2.d) High capacity and high data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ADRLw4i4cO"
      },
      "source": [
        "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase set the number of hidden units to 12 and the number of training points to 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EWpipC2i72U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7AJjSURi-Qx"
      },
      "source": [
        "**TODO:** Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwFGSKRIjHgd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdwDgFvrjFjV"
      },
      "source": [
        "**TODO**: Record your observations about the bias and variance in this setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1Hf1QJjQKg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ce1iSIwamXh"
      },
      "source": [
        "## Q3) High Dimensional Spaces\n",
        "This question investigates the strange properties of high-dimensional spaces as discussed in the notes at the end of chapter 8.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xfTUNQvcPx-"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRRwkYqVan88"
      },
      "outputs": [],
      "source": [
        "# Fix the random seed so we all have the same random numbers\n",
        "np.random.seed(0)\n",
        "n_data = 1000\n",
        "# Create 1000 data examples (columns) each with 2 dimensions (rows)\n",
        "n_dim = 2\n",
        "x_2D = np.random.normal(size=(n_dim,n_data))\n",
        "# Create 1000 data examples (columns) each with 100 dimensions (rows)\n",
        "n_dim = 100\n",
        "x_100D = np.random.normal(size=(n_dim,n_data))\n",
        "# Create 1000 data examples (columns) each with 1000 dimensions (rows)\n",
        "n_dim = 1000\n",
        "x_1000D = np.random.normal(size=(n_dim,n_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlY5Ewr9d9Kd"
      },
      "source": [
        "**TODO:** Implement the missing parts of the function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRrYxiAIaxHp"
      },
      "outputs": [],
      "source": [
        "def distance_ratio(x):\n",
        "  # TODO -- replace the two lines below to calculate the largest and smallest Euclidean distance between\n",
        "  # the data points in the columns of x.  DO NOT include the distance between the data point\n",
        "  # and itself (which is obviously zero)\n",
        "\n",
        "  smallest_dist = 1.0\n",
        "  largest_dist = 1.0\n",
        "\n",
        "  # Calculate the ratio and return\n",
        "  dist_ratio = largest_dist / smallest_dist\n",
        "  return dist_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of_ivtxgayPp"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('Ratio of largest to smallest distance 2D: %3.3f'%(distance_ratio(x_2D)))\n",
        "print('Ratio of largest to smallest distance 100D: %3.3f'%(distance_ratio(x_100D)))\n",
        "print('Ratio of largest to smallest distance 1000D: %3.3f'%(distance_ratio(x_1000D)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKlKDJ5ld0HT"
      },
      "source": [
        "If you did this right, you will see that the distance between the nearest and farthest two points in high dimensions is almost the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmyrvE3Ljzt7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}