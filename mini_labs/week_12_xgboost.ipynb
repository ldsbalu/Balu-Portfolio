{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7FlusfzZ_8q"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/mini_labs/week_12_xgboost.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# BYU CS 473 — XGBoost\n",
        "\n",
        "In this assignment, you will learn the core ideas behind XGBoost and apply the method to a dataset of your choice.\n",
        "We’ll connect the math from the textbook to hands-on modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Goals\n",
        "- Explain the XGBoost objective function and its components.\n",
        "- Define and use key terms: regularizer, second-order Taylor expansion, leaf weights, gain, and split criterion.\n",
        "- Apply XGBoost to a dataset, tune hyperparameters, and evaluate results.\n",
        "- Understand how XGBoost improves upon traditional boosting methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvClpS40afIy"
      },
      "source": [
        "## Part 1 — Key Concepts from the Textbook  \n",
        "\n",
        "Read through the definitions below. For each one, write a **1–2 sentence explanation in your own words**.  \n",
        "\n",
        "### 1. Regularizer  \n",
        "Equation (18.47):  \n",
        "$\\Omega(f) = \\gamma J + \\frac{1}{2} \\lambda \\sum_{j=1}^J w_j^2$  \n",
        "\n",
        "**Question:** Why does XGBoost penalize both the **number of leaves** and the **magnitude of leaf weights**?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FSHTaM5bnL5"
      },
      "source": [
        "Regularizer helps in penalizing leafs and weight of it to avoid complexity in tree which can lead to overfit in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7u8d1fPbo1Z"
      },
      "source": [
        "### 2. Second-order Taylor Expansion of the Loss  \n",
        "Equation (18.49):  \n",
        "$L_m(F_m) \\approx \\sum_{i=1}^N \\Big[ \\ell(y_i, f_{m-1}(x_i)) + g_{im} F_m(x_i) + \\tfrac{1}{2} h_{im} F_m(x_i)^2 \\Big] + \\Omega(F_m)$  \n",
        "\n",
        "**Question:** How does including the **Hessian term** (curvature) make boosting more accurate compared to using only gradients?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbcMbhoObxfG"
      },
      "source": [
        "Second-order Taylor Expansion of the Loss uses curvature information for better optimization which lead to more perfect and proper updates than gradient-only methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtO80EYbyzT"
      },
      "source": [
        "### 3. Optimal Leaf Weights  \n",
        "Equation (18.54):  \n",
        "$w_j^* = - \\frac{G_{jm}}{H_{jm} + \\lambda}$  \n",
        "\n",
        "**Question:** What does this formula mean about how leaf weights are chosen?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8yhL2Vb1Em"
      },
      "source": [
        "The formula for optimal leaf weights shows that each leaf's score is determined by the sum of gradients and curvature within that leaf, it is normalized by a regularized denominator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4puP4SLlb5x6"
      },
      "source": [
        "### 4. Gain of a Split  \n",
        "Equation (18.56):  \n",
        "$\\text{gain} = \\tfrac{1}{2}\\Bigg( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\Bigg) - \\gamma$  \n",
        "\n",
        "**Question:** Why does XGBoost reject splits with **negative gain**?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UNat8V3cFQY"
      },
      "source": [
        "Since XGboost doesnt do any better in improve objective of the model. It rejects splits with negative gain an they add complexity without reducing the loss sufficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_SetVjcHmW"
      },
      "source": [
        "## Part 2 — Visualizing Boosting  \n",
        "\n",
        "### 2.1 Bagging vs Boosting (Recap)  \n",
        "Fescribe in words how **bagging** and **boosting** differ in how they:  \n",
        "- Use data sampling  \n",
        "- Build models sequentially or in parallel  \n",
        "- Reduce bias vs variance  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging samples data with replacement and builds models in parallel to reduce variance, while boosting trains models sequentially, with each new model focusing on correcting errors of the previous ones, reducing bias."
      ],
      "metadata": {
        "id": "utw9uJHl-NgU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TB0dxSKcoC-"
      },
      "source": [
        "## Part 3 — Implementing XGBoost on 2 Datasets  \n",
        "\n",
        "### Step 1 — Look at the example dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihoCOlQ_dab8",
        "outputId": "8a426a76-14c5-41df-bdbb-6136bc1f052c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "# Example: load a dataset\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = xgb.XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    eta=0.1,        # learning rate\n",
        "    max_depth=3,    # tree depth\n",
        "    n_estimators=100,\n",
        "    reg_lambda=1.0, # L2 regularization\n",
        "    reg_alpha=0.0   # L1 regularization\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQZdb9NepPA"
      },
      "source": [
        "### Step 2 — Implement XGboost on a dataset of your choice  \n",
        "- Example locations to find a dataset:  \n",
        "  - A built-in dataset (e.g. `load_digits`)  \n",
        "  - A Kaggle dataset  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = xgb.XGBClassifier(objective=\"multi:softmax\", eval_metric=\"mlogloss\", eta=0.1, max_depth=3,\n",
        "                          n_estimators=100, reg_lambda=1.0, reg_alpha=0.0, num_class=10)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Digits Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zryckltvAbJi",
        "outputId": "2e1cba76-f39e-498c-ea56-e0b5e077c58f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits Accuracy: 0.9638888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOihhCHdcAq"
      },
      "source": [
        "### Step 3 — Experiment with Hyperparameters on your dataset and the Cancer dataset\n",
        "- Change `max_depth`, `eta`, or `n_estimators`.  \n",
        "- Add regularization with `reg_lambda` and `reg_alpha`.  \n",
        "- **Question:** How do these changes affect performance?  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = xgb.XGBClassifier(eta=0.05, max_depth=6, n_estimators=200, reg_lambda=2.0, reg_alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Tuned Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raGtBctYA4SK",
        "outputId": "b8b05da1-50a5-421f-926e-4af641cdffb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Accuracy: 0.9527777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5aD9xqkdgtR"
      },
      "source": [
        "Higher depth and more estimators can increase accuracy but may cause overfitting\n",
        "regularization can help but may underfit if we set it too high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJn4fMq_eTGx"
      },
      "source": [
        "## Part 4 — Reflection  \n",
        "\n",
        "Answer the following in complete sentences:  \n",
        "1. What role does the **regularizer** play in preventing overfitting?  \n",
        "2. How does using the **second-order Taylor expansion** help optimize the trees?  \n",
        "3. What surprised you most when experimenting with hyperparameters?  \n",
        "4. Why is XGBoost considered both a **statistical innovation** (Taylor expansion, regularization) and a **computer science innovation** (scalability, out-of-core learning)?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnaftVXVeUIN"
      },
      "source": [
        "1. Regularizer helps in preventing overly complex trees and large weights, which helps in model to generalize better.\n",
        "2. Second-order Taylor Expansion of the Loss uses curvature information for better optimization which lead to more perfect and proper updates than gradient-only methods\n",
        "3. Tweaking hypermeters often reveals that there is a sweet spot but it may effect accuracy through regularization.\n",
        "4.  XGBoost is a statistical innovation because it has ability to advance the  optimization, and a computer science innovation due to its computational efficiency, scalability, and ability to handle very large dataset\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}